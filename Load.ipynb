{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from unrar import rarfile\n",
    "import zipfile\n",
    "import io\n",
    "from contextlib import closing\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# Helper function, adapted from\n",
    "# https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065\n",
    "def find_runs(x):\n",
    "    \"\"\"Find runs of consecutive items in an array.\"\"\"\n",
    "\n",
    "    # ensure array\n",
    "    x = np.asanyarray(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError('only 1D array supported')\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # handle empty array\n",
    "    if n == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    else:\n",
    "        # find run starts\n",
    "        loc_run_start = np.empty(n, dtype=bool)\n",
    "        loc_run_start[0] = True\n",
    "        np.not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n",
    "        run_starts = np.nonzero(loc_run_start)[0]\n",
    "\n",
    "        # find run values\n",
    "        run_values = x[loc_run_start]\n",
    "\n",
    "        # find run lengths\n",
    "        run_lengths = np.diff(np.append(run_starts, n))\n",
    "\n",
    "        return run_values, run_starts, run_lengths\n",
    "\n",
    "def split_longer_runs(old_labels, old_starts, old_lengths, min_split_length=30):\n",
    "    new_labels, new_starts, new_lengths = [], [], []\n",
    "\n",
    "    for i in range(len(old_labels)):\n",
    "        sequence_split_count = old_lengths[i] // min_split_length\n",
    "\n",
    "        if sequence_split_count == 0:\n",
    "            new_labels.append(old_labels[i])\n",
    "            new_starts.append(old_starts[i])\n",
    "            new_lengths.append(old_lengths[i])\n",
    "            continue\n",
    "\n",
    "        sequence_split_length = old_lengths[i] // sequence_split_count\n",
    "\n",
    "        for j in range(sequence_split_count):\n",
    "            new_labels.append(old_labels[i])\n",
    "            new_starts.append(old_starts[i] + j * sequence_split_length)\n",
    "            new_lengths.append(sequence_split_length)\n",
    "\n",
    "    return np.array(new_labels, dtype=np.int64), \\\n",
    "            np.array(new_starts, dtype=np.int64), \\\n",
    "            np.array(new_lengths, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load uWave dataset\n",
    "\n",
    "Dataset is available at http://zhen-wang.appspot.com/rice/files/uwave/uWaveGestureLibrary.zip\n",
    "Redistribution for this repository was explicitly allowed by\n",
    "the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'http://zhen-wang.appspot.com/rice/files/uwave/uWaveGestureLibrary.zip'\n",
    "r = requests.get(DATA_URL)\n",
    "files_in_zip = {}\n",
    "with closing(r), zipfile.ZipFile(io.BytesIO(r.content)) as archive:\n",
    "    for member in archive.infolist():\n",
    "        files_in_zip[member.filename] = archive.read(member)\n",
    "\n",
    "class_ids = list(range(8))\n",
    "class_labels = ['diagonal', 'square', 'right', 'left',\n",
    "                'up', 'down', 'clockwise', 'counter clockwise']\n",
    "# person * day * class * attempt\n",
    "instance_count = 8 * 7 * 8 * 10\n",
    "# maximal length of gesture in dataset - rest is zero padded\n",
    "max_length = 315\n",
    "\n",
    "with h5py.File('uwave.h5', 'a') as h5f:\n",
    "    for key in ['x', 'y', 'meta']:\n",
    "        if key in h5f:\n",
    "            del h5f[key]\n",
    "    xs = h5f.create_dataset('x', (instance_count, max_length, 3), dtype='float64')\n",
    "    xs.attrs['d1'] = 'instance'\n",
    "    xs.attrs['d2'] = 'time'\n",
    "    xs.attrs['d3'] = 'channel'\n",
    "    xs.attrs['channels'] = ['acc1-x', 'acc1-y', 'acc1-z']\n",
    "\n",
    "    ys = h5f.create_dataset('y', (instance_count, ), dtype=np.dtype([('class', np.int8)]))\n",
    "    ys.attrs['d1'] = 'instance'\n",
    "    ys.attrs['classes'] = class_ids\n",
    "    ys.attrs['labels'] = class_labels\n",
    "\n",
    "\n",
    "    metas = h5f.create_dataset('meta', (instance_count, ),\n",
    "                               dtype=np.dtype([('user', np.int8), ('day', np.int8), ('attempt', np.int8), ('length', np.int32)]))\n",
    "    metas.attrs['d1'] = 'instance'\n",
    "\n",
    "    txt_regex = re.compile('[A-Z]_\\w*_\\w*(\\d+)-(\\d+).txt')\n",
    "    count = 0\n",
    " \n",
    "    for user_id in range(1, 9):\n",
    "        for day_id in range(1, 8):\n",
    "            rar_content = files_in_zip[f'U{user_id} ({day_id}).rar']\n",
    "\n",
    "            with tempfile.NamedTemporaryFile() as f:\n",
    "                f.write(rar_content)\n",
    "                f.flush()\n",
    "                with rarfile.RarFile(f.name) as rf:\n",
    "                    rf.testrar()\n",
    "                    for f in rf.infolist():\n",
    "                        match = txt_regex.match(f.filename)\n",
    "                        if match:\n",
    "                            target_class, attempt = match.group(1), match.group(2)\n",
    "                            with rf.open(f.filename) as txt_file:\n",
    "                                instance_data = np.loadtxt(txt_file).reshape(-1, 3)\n",
    "                                padded_data = np.pad(instance_data, ((0, max_length - instance_data.shape[0]), (0,0)), 'constant')\n",
    "                                xs[count] = padded_data\n",
    "                                ys[count] = int(target_class) - 1\n",
    "                                metas[count] = (user_id, day_id, attempt, instance_data.shape[0])\n",
    "\n",
    "                            count += 1\n",
    "    ys.attrs['classes'] = np.unique(ys).astype(np.int8)\n",
    "# Basic sanity check\n",
    "assert count == instance_count            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Skoda dataset\n",
    "\n",
    "Dataset is available at http://har-dataset.org/lib/exe/fetch.php?media=wiki:dataset:skodaminicp:skodaminicp_2015_08.zip\n",
    "\n",
    "After downloading, it should be unzipped into the `original` directory in the project root.\n",
    "\n",
    "Be aware of the fact, that we cut off the activities to be of length 1163 (this cuts\n",
    "one outlier activity of length 1713)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis 0 processed.\n",
      "Axis 1 processed.\n",
      "Axis 2 processed.\n",
      "Axis 3 processed.\n",
      "Axis 4 processed.\n",
      "Axis 5 processed.\n",
      "Axis 6 processed.\n",
      "Axis 7 processed.\n",
      "Axis 8 processed.\n",
      "Axis 9 processed.\n",
      "Axis 10 processed.\n",
      "Axis 11 processed.\n",
      "Axis 12 processed.\n",
      "Axis 13 processed.\n",
      "Axis 14 processed.\n",
      "Axis 15 processed.\n",
      "Axis 16 processed.\n",
      "Axis 17 processed.\n",
      "Axis 18 processed.\n",
      "Axis 19 processed.\n",
      "Axis 20 processed.\n",
      "Axis 21 processed.\n",
      "Axis 22 processed.\n",
      "Axis 23 processed.\n",
      "Axis 24 processed.\n",
      "Axis 25 processed.\n",
      "Axis 26 processed.\n",
      "Axis 27 processed.\n",
      "Axis 28 processed.\n",
      "Axis 29 processed.\n",
      "Axis 30 processed.\n",
      "Axis 31 processed.\n",
      "Axis 32 processed.\n",
      "Axis 33 processed.\n",
      "Axis 34 processed.\n",
      "Axis 35 processed.\n",
      "Axis 36 processed.\n",
      "Axis 37 processed.\n",
      "Axis 38 processed.\n",
      "Axis 39 processed.\n",
      "Axis 40 processed.\n",
      "Axis 41 processed.\n",
      "Axis 42 processed.\n",
      "Axis 43 processed.\n",
      "Axis 44 processed.\n",
      "Axis 45 processed.\n",
      "Axis 46 processed.\n",
      "Axis 47 processed.\n",
      "Axis 48 processed.\n",
      "Axis 49 processed.\n",
      "Axis 50 processed.\n",
      "Axis 51 processed.\n",
      "Axis 52 processed.\n",
      "Axis 53 processed.\n",
      "Axis 54 processed.\n",
      "Axis 55 processed.\n",
      "Axis 56 processed.\n",
      "Axis 57 processed.\n",
      "Axis 58 processed.\n",
      "Axis 59 processed.\n"
     ]
    }
   ],
   "source": [
    "skoda_mat = sio.loadmat('../original/SkodaMiniCP_2015_08/dataset_cp_2007_12.mat')\n",
    "\n",
    "class_ids = list(range(10))\n",
    "class_labels = ['write notepad', 'open hood', 'close hood',\n",
    "                'check gaps front', 'open left front door',\n",
    "                'close left front door', 'close both left door',\n",
    "                'check trunk gaps', 'open and close trunk',\n",
    "                'check steering wheel']\n",
    "instance_count = 4867\n",
    "max_length = 177\n",
    "\n",
    "with h5py.File('skoda.h5', 'a') as h5f:\n",
    "    for key in ['x', 'y', 'meta']:\n",
    "        if key in h5f:\n",
    "            del h5f[key]\n",
    "    xs = h5f.create_dataset('x', (instance_count, max_length, 60), dtype='float64')\n",
    "    xs.attrs['d1'] = 'instance'\n",
    "    xs.attrs['d2'] = 'time'\n",
    "    xs.attrs['d3'] = 'channel'\n",
    "    xs.attrs['channels'] = sum(\n",
    "        [\n",
    "            [f'acc{i}-x', f'acc{i}-y', f'acc{i}-z']\n",
    "            for i in range(1, 21)\n",
    "        ], [])\n",
    "\n",
    "    ys = h5f.create_dataset('y', (instance_count, ), dtype=np.dtype([('class', np.int8)]))\n",
    "    ys.attrs['d1'] = 'instance'\n",
    "    ys.attrs['classes'] = class_ids\n",
    "    ys.attrs['labels'] = class_labels\n",
    "\n",
    "    metas = h5f.create_dataset('meta', (instance_count, ), dtype=np.dtype([('attempt', np.int8), ('length', np.int32)]))\n",
    "    metas.attrs['d1'] = 'instance'\n",
    "    \n",
    "    rlengths = []\n",
    "    rrlengths = []\n",
    "    \n",
    "    for axis, axis_data in enumerate(skoda_mat['dataset_left'][0]):\n",
    "        cum_offset = 0\n",
    "        for target_class, target_class_data in enumerate(axis_data[0]):\n",
    "            for instance, instance_data in enumerate(target_class_data[0][:70]):\n",
    "                instance_data = instance_data[0]\n",
    "                labels, starts, lengths = split_longer_runs([target_class], [0], [instance_data.shape[0]], min_split_length=100)\n",
    "                \n",
    "                for offset in range(len(labels)):\n",
    "                    split_instance_data = instance_data[starts[offset]: starts[offset] + lengths[offset]]\n",
    "                    padded_data = np.pad(split_instance_data, ((0, max_length - split_instance_data.shape[0])), 'constant')\n",
    "                                                                \n",
    "                    xs[cum_offset + offset, :, axis] = padded_data\n",
    "                    ys[cum_offset + offset] = target_class\n",
    "                    metas[cum_offset + offset] = (instance, split_instance_data.shape[0])\n",
    "                rlengths.append(len(labels))\n",
    "\n",
    "                cum_offset += len(labels)\n",
    "        assert cum_offset == instance_count\n",
    "        print(f'Axis {axis} processed.')\n",
    "\n",
    "        \n",
    "    for axis, axis_data in enumerate(skoda_mat['dataset_right'][0]):\n",
    "        cum_offset = 0\n",
    "        for target_class, target_class_data in enumerate(axis_data[0]):\n",
    "            for instance, instance_data in enumerate(target_class_data[0][:70]):\n",
    "                otherside_length = skoda_mat['dataset_left'][0][axis][0][target_class][0][instance][0].shape[0]\n",
    "                instance_data = instance_data[0][:otherside_length]\n",
    "                # Cut and pad to same length as other side\n",
    "                instance_data = np.pad(instance_data, ((0, otherside_length - instance_data.shape[0])), 'constant')\n",
    "                \n",
    "                labels, starts, lengths = split_longer_runs([target_class], [0], [instance_data.shape[0]], min_split_length=100)\n",
    "                \n",
    "                for offset in range(len(labels)):\n",
    "                    split_instance_data = instance_data[starts[offset]: starts[offset] + lengths[offset]]\n",
    "                    padded_data = np.pad(split_instance_data, ((0, max_length - split_instance_data.shape[0])), 'constant')\n",
    "                                                                \n",
    "                    xs[cum_offset + offset, :, 30 + axis] = padded_data\n",
    "\n",
    "                cum_offset += len(labels)\n",
    "                rrlengths.append(len(labels))\n",
    "        assert cum_offset == instance_count\n",
    "        print(f'Axis {30 + axis} processed.')\n",
    "    ys.attrs['classes'] = np.unique(ys).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Opportunity dataset\n",
    "\n",
    "Dataset is available at http://opportunity-project.eu/challengeDownload.html\n",
    "\n",
    "After downloading, it should be unzipped into the `original` directory in the project root.\n",
    "\n",
    "We use the channels suggested by the original documentation (2-37, 38-46, 51-59, 64-72, 77-85, 90-98, 103-134)\n",
    "and use the middle level gesture labels as target classes.\n",
    "\n",
    "Meta contains information about the subject and run.\n",
    "\n",
    "Beware that the data contains nan values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : S1-ADL4.dat\n",
      "Processing : S1-Drill.dat\n",
      "Processing : S1-ADL5.dat\n",
      "Processing : S1-ADL1.dat\n",
      "Processing : S1-ADL2.dat\n",
      "Processing : S1-ADL3.dat\n",
      "Processing : S2-ADL2.dat\n",
      "Processing : S3-ADL2.dat\n",
      "Processing : S3-ADL3.dat\n",
      "Processing : S2-ADL3.dat\n",
      "Processing : S3-ADL1.dat\n",
      "Processing : S2-ADL1.dat\n",
      "Processing : S3-Drill.dat\n",
      "Processing : S3-ADL4.dat\n",
      "Processing : S2-ADL4.dat\n",
      "Processing : S2-ADL5.dat\n",
      "Processing : S3-ADL5.dat\n",
      "Processing : S4-ADL4.dat\n",
      "Processing : S4-ADL5.dat\n",
      "Processing : S2-Drill.dat\n",
      "Processing : S4-ADL2.dat\n",
      "Processing : S4-ADL3.dat\n",
      "Processing : S4-ADL1.dat\n",
      "Processing : S4-Drill.dat\n"
     ]
    }
   ],
   "source": [
    "opportunity_data_dir = '../original/OpportunityUCIDataset/dataset'\n",
    "\n",
    "label_translation = {\n",
    "    0: 0,\n",
    "    406516: 1, 406517: 2, 404516: 3, 404517: 4,\n",
    "    406520: 5, 404520: 6, 406505: 7, 404505: 8, 406519: 9, 404519: 10,\n",
    "    406511: 11, 404511: 12, 406508: 13, 404508: 14,\n",
    "    408512: 15, 407521: 16, 405506: 17\n",
    "}\n",
    "class_ids = list(range(18))\n",
    "class_labels = ['null', 'open door 1', 'open door 2', 'close door 1',\n",
    "                'close door 2', 'open fridge', 'close fridge',\n",
    "                'open dishwasher', 'close dishwasher',\n",
    "                'open drawer 1', 'close drawer 1',\n",
    "                'open drawer 2', 'close drawer 2',\n",
    "                'open drawer 3', 'close drawer 3',\n",
    "                'clean table', 'drink from cup', 'toggle switch']\n",
    "instance_count = 27530\n",
    "max_length = 60\n",
    "\n",
    "channels = []\n",
    "for sensor in ['RKN^', 'HIP', 'LUA^', 'RUA_', 'LH', 'BACK', 'RKN_', 'RWR', 'RUA^', 'LUA_', 'LWR', 'RH']:\n",
    "    for axis in ['accX', 'accY', 'accZ']:\n",
    "        channels.append(f'Accelerometer {sensor} {axis}')\n",
    "\n",
    "for sensor in ['BACK', 'RUA', 'RLA', 'LUA', 'LLA']:\n",
    "    for axis in ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', 'magneticX', 'magneticY', 'magneticZ']:\n",
    "        channels.append(f'InertialMeasurementUnit {sensor} {axis}')\n",
    "\n",
    "for sensor in ['L-SHOE', 'R-SHOE']:\n",
    "    for axis in ['EuX', 'EuY', 'EuZ', 'Nav_Ax', 'Nav_Ay', 'Nav_Az', 'Body_Ax', 'Body_Ay', 'Body_Az', 'AngVelBodyFrameX', 'AngVelBodyFrameY', 'AngVelBodyFrameZ', 'AngVelNavFrameX', 'AngVelNavFrameY', 'AngVelNavFrameZ', 'Compass']:\n",
    "        channels.append(f'InertialMeasurementUnit {sensor} {axis}')\n",
    "\n",
    "with h5py.File('opportunity.h5', 'a') as h5f:\n",
    "    for key in ['x', 'y', 'meta']:\n",
    "        if key in h5f:\n",
    "            del h5f[key]\n",
    "    xs = h5f.create_dataset('x', (instance_count, max_length, 113), dtype='float64')\n",
    "    xs.attrs['d1'] = 'instance'\n",
    "    xs.attrs['d2'] = 'time'\n",
    "    xs.attrs['d3'] = 'channel'\n",
    "    xs.attrs['channels'] = channels\n",
    "\n",
    "    ys = h5f.create_dataset('y', (instance_count, ), dtype=np.dtype([('class', np.int8)]))\n",
    "    ys.attrs['d1'] = 'instance'\n",
    "    ys.attrs['classes'] = class_ids\n",
    "    ys.attrs['labels'] = class_labels\n",
    "\n",
    "    metas = h5f.create_dataset('meta', (instance_count, ), dtype=np.dtype([('subject', np.int8), ('length', np.int32), ('run', 'S10')]))\n",
    "    metas.attrs['d1'] = 'instance'\n",
    "    count = 0\n",
    "    for file in os.listdir(opportunity_data_dir):\n",
    "        if not file.endswith('.dat'):\n",
    "            continue\n",
    "        print(f'Processing : {file}')\n",
    "        data = np.loadtxt(os.path.join(opportunity_data_dir, file))\n",
    "        subject_id = int(file[1]) - 1\n",
    "        run_id = file[3:-4]\n",
    "        input_channels = np.concatenate([data[:, 1:37], data[:,37:46],\n",
    "                              data[:,50:59], data[:,63:72], data[:,76:85],\n",
    "                              data[:,89:98], data[:,102:134] ], axis=1)\n",
    "        labels, starts, lengths = split_longer_runs(*find_runs(data[:, 249]))\n",
    "        for offset in range(len(labels)):\n",
    "            instance_data = input_channels[starts[offset]: starts[offset] + lengths[offset]]\n",
    "            padded_data = np.pad(instance_data, ((0, max_length - instance_data.shape[0]), (0,0)), 'constant')\n",
    "            xs[count + offset] = padded_data\n",
    "            ys[count + offset] = label_translation[labels[offset]]\n",
    "            metas[count + offset] = (subject_id, lengths[offset], run_id)\n",
    "\n",
    "        count += len(labels)\n",
    "\n",
    "assert count == instance_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PAMAP2 dataset\n",
    "\n",
    "Dataset is available at https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring\n",
    "\n",
    "After downloading, it should be unzipped into the `original` directory in the project root.\n",
    "\n",
    "We use the channels for the heart rate, the 16g range acceleration, gyroscope data\n",
    "and magnetometer data, as noted in the original documentation,\n",
    "from both the Protocol and Optional runs.\n",
    "\n",
    "Meta contains information about the subject.\n",
    "\n",
    "Beware that the data contains nan values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : subject108.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Processing : subject109.dat\n",
      "Found segments of classes: [ 0 24]\n",
      "Processing : subject107.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17]\n",
      "Processing : subject106.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Processing : subject104.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17]\n",
      "Processing : subject105.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Processing : subject101.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Processing : subject102.dat\n",
      "Found segments of classes: [ 0  1  2  3  4  5  6  7 12 13 16 17 24]\n",
      "Processing : subject103.dat\n",
      "Found segments of classes: [ 0  1  2  3  4 12 13 16 17]\n",
      "Processing : subject108.dat\n",
      "Found segments of classes: [ 0 10 18 19 20]\n",
      "Processing : subject109.dat\n",
      "Found segments of classes: [ 0 10 18 19 20]\n",
      "Processing : subject106.dat\n",
      "Found segments of classes: [ 0 10 18 19]\n",
      "Processing : subject105.dat\n",
      "Found segments of classes: [ 0 10 19]\n",
      "Processing : subject101.dat\n",
      "Found segments of classes: [ 0  9 11 18 19]\n"
     ]
    }
   ],
   "source": [
    "pamap_data_dir = '../original/PAMAP2_Dataset/Protocol'\n",
    "pamap_data_dir_2 = '../original/PAMAP2_Dataset/Optional'\n",
    "label_translation = {\n",
    "    1:0, 2:1, 3:2, 4:3, 5:4, 6:5, 7:6, 9:7, 10:8,\n",
    "    11:9, 12:10, 13:11, 16:12, 17:13, 18:14, 19:15, 20:16, 24:17,\n",
    "}\n",
    "class_ids = list(range(18))\n",
    "class_labels = ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling',\n",
    "                'nordic walking', 'watching tv', 'computer work', 'car driving',\n",
    "                'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing',\n",
    "                'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n",
    "\n",
    "instance_count = 27185\n",
    "max_length = 128\n",
    "\n",
    "channels = ['heartrate']\n",
    "for sensor in range(1, 4):\n",
    "    for modality in ['acc', 'gyro', 'magnet']:\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            channels.append(f'{modality}{sensor}-{axis}')\n",
    "\n",
    "with h5py.File('pamap2.h5', 'a') as h5f:\n",
    "    for key in ['x', 'y', 'meta']:\n",
    "        if key in h5f:\n",
    "            del h5f[key]\n",
    "    xs = h5f.create_dataset('x', (instance_count, max_length, len(channels)), dtype='float64')\n",
    "    xs.attrs['d1'] = 'instance'\n",
    "    xs.attrs['d2'] = 'time'\n",
    "    xs.attrs['d3'] = 'channel'\n",
    "    xs.attrs['channels'] = channels\n",
    "\n",
    "    ys = h5f.create_dataset('y', (instance_count, ), dtype=np.dtype([('class', np.int8)]))\n",
    "    ys.attrs['d1'] = 'instance'\n",
    "    ys.attrs['classes'] = class_ids\n",
    "    ys.attrs['labels'] = class_labels\n",
    "\n",
    "    metas = h5f.create_dataset('meta', (instance_count, ), dtype=np.dtype([('subject', np.int8), ('length', np.int32)]))\n",
    "    metas.attrs['d1'] = 'instance'\n",
    "    count = 0\n",
    "    files = os.listdir(pamap_data_dir) + os.listdir(pamap_data_dir_2)\n",
    "    paths = [os.path.join(pamap_data_dir, file) for file in os.listdir(pamap_data_dir)] \\\n",
    "            + [os.path.join(pamap_data_dir_2, file) for file in os.listdir(pamap_data_dir_2)]\n",
    "    for file, path in zip(files, paths):\n",
    "        if not file.endswith('.dat'):\n",
    "            continue\n",
    "        print(f'Processing : {file}')\n",
    "        data = np.loadtxt(path)\n",
    "        subject_id = int(file[7:-4]) - 101\n",
    "        input_channels = np.concatenate([data[:, 2:3],\n",
    "                              data[:,4:7], data[:,10:16],\n",
    "                              data[:,21:24], data[:,27:33],\n",
    "                              data[:,38:41], data[:,44:50]], axis=1)\n",
    "        labels, starts, lengths = split_longer_runs(*find_runs(data[:, 1]), 100)\n",
    "        print('Found segments of classes:', np.unique(labels))\n",
    "        inc_count = 0\n",
    "        for offset in range(len(labels)):\n",
    "            if labels[offset] == 0:\n",
    "                continue  # Skip the transient activity data\n",
    "\n",
    "            instance_data = input_channels[starts[offset]: starts[offset] + lengths[offset]]\n",
    "            padded_data = np.pad(instance_data, ((0, max_length - instance_data.shape[0]), (0,0)), 'constant')\n",
    "            xs[count + inc_count] = padded_data\n",
    "            ys[count + inc_count] = label_translation[labels[offset]]\n",
    "            metas[count + inc_count] = (subject_id, lengths[offset])\n",
    "            inc_count += 1\n",
    "        count += inc_count\n",
    "\n",
    "assert count == instance_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets preprocessed by Jordao et al.\n",
    "\n",
    "These preprocessed datasets are available at\n",
    "https://github.com/arturjordao/WearableSensorData\n",
    "\n",
    "The git repository should be cloned into the `original` directory inside the project root.\n",
    "\n",
    "Data is read from the numpy format and re-saved into hdf5.\n",
    "- target class is translated from one-hot encoding to integers\n",
    "- we use the FNOW directory (fully non overlapping window), to comply with the rest\n",
    "  of our datasets\n",
    "\n",
    "The original work also contains pre-calculated folds used for reproduction,\n",
    "however these folds are not passed on to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : USCHAD.npz\n",
      "5038\n",
      "Processing : UTD-MHAD1_1s.npz\n",
      "2048\n",
      "Processing : WISDM.npz\n",
      "10516\n",
      "Processing : WHARF.npz\n",
      "2146\n",
      "Processing : UTD-MHAD2_1s.npz\n",
      "616\n",
      "Processing : MHEALTH.npz\n",
      "1335\n"
     ]
    }
   ],
   "source": [
    "wsd_data_dir = '../original/WearableSensorData/data/FNOW'\n",
    "\n",
    "for file in os.listdir(wsd_data_dir):\n",
    "    if not file.endswith('.npz'):\n",
    "        continue\n",
    "    print(f'Processing : {file}')\n",
    "    data = np.load(os.path.join(wsd_data_dir, file))\n",
    "    in_X = data['X']\n",
    "    in_y = data['y']\n",
    "\n",
    "    instance_count = in_X.shape[0]\n",
    "    max_length = in_X.shape[2]\n",
    "    channel_count = in_X.shape[3]\n",
    "\n",
    "    with h5py.File(f'{file[:-4].lower()}.h5', 'a') as h5f:\n",
    "        for key in ['x', 'y', 'meta']:\n",
    "            if key in h5f:\n",
    "                del h5f[key]\n",
    "        xs = h5f.create_dataset('x', (instance_count, max_length, channel_count),\n",
    "                                dtype='float64',\n",
    "                                data=in_X[:, 0, :, :])\n",
    "        xs.attrs['d1'] = 'instance'\n",
    "        xs.attrs['d2'] = 'time'\n",
    "        xs.attrs['d3'] = 'channel'\n",
    "        \n",
    "        t = in_y.argmax(axis=1)\n",
    "        print(instance_count)\n",
    "        ys = h5f.create_dataset('y', (instance_count, ), dtype=np.dtype([('class', np.int8)]))\n",
    "        ys[:] = in_y.argmax(axis=1)[:]\n",
    "        ys.attrs['d1'] = 'instance'\n",
    "        ys.attrs['classes'] = np.unique(ys).astype(np.int8)\n",
    "\n",
    "        metas = h5f.create_dataset('meta', (instance_count, ),\n",
    "                                   dtype=np.dtype([('length', np.int32)]))\n",
    "        metas[:] = [max_length] * instance_count\n",
    "        metas.attrs['d1'] = 'instance'\n",
    "        \n",
    "        if file == 'MHEALTH.npz':\n",
    "            xs.attrs['channels'] = ['acc1-x', 'acc1-y', 'acc1-z', 'ecg1', 'ecg2',\n",
    "                                   'acc2-x', 'acc2-y', 'acc2-z', 'gyro2-x', 'gyro2-y', 'gyro2-z', 'magnet2-x', 'magnet2-y', 'magnet2-z',\n",
    "                                   'acc3-x', 'acc3-y', 'acc3-z', 'gyro3-x', 'gyro3-y', 'gyro3-z', 'magnet3-x', 'magnet3-y', 'magnet3-z']\n",
    "            ys.attrs['labels'] = ['standing still', 'sitting and relaxing', 'lying down', 'walking', 'climbing stairs',\n",
    "                                 'waist bends forward', 'frontal elevation of arms', 'knees bending', 'cycling', 'jogging',\n",
    "                                 'running', 'jump front & back']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gesture]",
   "language": "python",
   "name": "conda-env-gesture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
